\chapter{Conclusion and Future Work}

\section{Conclusion}
This dissertation presents our research work on shape modeling and 
analysis based on spectral representations and sparsity-driven algorithms. 
Our proposed methods provide solutions to a wide range of shape modeling 
problems with competitive performance, demonstrating the great potential
of spectral methods and sparse representations in the mesh domain.

We present an innovative approach to 3D mesh compression
by incorporating redundant spectral graph wavelets in dictionary design and using
greedy pursuits to find compressed coefficient approximation.

By formulating surface inpainting as a sparse signal recovery problem,
we propose a novel variational approach for surface inpainting, integrating data fidelity
constraints on the shape domain with coefficient sparsity constraints on the
transformed domain.

We propose a novel informative shape descriptor that combines SGW coefficients and
contour-based statistics for better characterization of feature shape regions. Based on
this descriptor, we develop a generalized feature detection framework, which can integrate different
types of regional descriptors to facilitate widespread applications including partial matching and
model recognition.

We develop an effective feature-driven articulated shape correspondence
algorithms. For coarse matching, we adopt
tensor-based high-order graph matching to maximizes the
geometric compatibility between features tuples, and for dense matching,
we present a hierarchical shape registration algorithm, generating
correspondence in multiples levels in a coarse-to-fine manner.

We develop effective feature-driven articulated shape
correspondence and retrieval algorithms based on 
isometry-invariant spectral representations. For coarse 
matching, we adopt tensor-based high-order graph matching to 
maximizes the geometric compatibility between features tuples, 
and for dense matching, we present a hierarchical shape 
registration algorithm, generating correspondence in multiples 
levels in a coarse-to-fine manner. Using the Bag-of-Feature-Graph (BoFG) 
descriptor for shape retrieval, we significantly reduce the number of 
pointed required for computing distributions in comparisons with 
more traditional Bag-of-Features descriptors. 

\section{Future Work}

There are many possible extensions and improvements related to our current research 
work in spectral representations and sparse modeling. Here we introduce some 
potential topics. 

\subsection*{Shape Editing}

Classical Laplacian mesh editing~\cite{Botsch2008} can be written as a regularized least square
problem. Taking the $x$-coordinate for example, soft constraint Laplacian mesh editing
constructs the new coordinate function by solving the following optimization problem
\begin{equation}
\min_\mathbf{x'} \sum_{j\in C} (x'_j - c_j)^2 + \gamma\|L\mathbf{x'}-L\mathbf{x}\|_2^2,
\end{equation}
where $L$ denotes Laplacian operator, $C$ represents the set of handle vertices, $c_j$ denotes the
coordinate of the handle vertex indexed at $j$, and $\mathbf{x'}$ and $\mathbf{x}$ represent the
original and deformed coordinates, respectively. This formulation ensures that the deformed shape
will preserve the second-order differential geometric information of the original shape while
conforming to the constraints of new handle positions.

Let function $\mathbf{f}=\mathbf{x'}-\mathbf{x}$ and denote the values of $\mathbf{f}$ at
handle vertices as $d_j = c_j - x_j, j\in C$. The optimization problem can be rewritten as
\begin{equation}
\label{eq:laplace-editing}
\min_\mathbf{f} \sum_{j\in C} (f_j - d_j)^2 + \gamma \mathbf{f}^T L^2 \mathbf{f}.
\end{equation}

Expanding $\mathbf{f}$ w.r.t. the Laplacian eigenbasis $\mathbf{\Phi}=\{\mathbf{\phi_k}\}$
as $\mathbf{f}=\sum_k a_k \mathbf{\phi_k}$, Eq.~\ref{eq:laplace-editing} changes to
\begin{equation}
\label{eq:Laplace-editing2}
\min_\mathbf{f} \sum_{j\in C} (f_j - d_j)^2 + \gamma \sum_k \lambda_k^2 a_k^2,
\end{equation}
where $\{\lambda_k\}$ denote the Laplacian eigenvalues.

Analyzing Eq.~\ref{eq:Laplace-editing2}, we can see that the formulation of Laplace
editing penalizing high frequency components (corresponding to larger
$\lambda_k$)~\cite{Rustamov:2009:ICMS}.

Replacing the bi-Laplacian in Eq.~\ref{eq:laplace-editing} with a spectral graph wavelet kernel
$W$ affords flexible design choices regarding the dominating frequencies. Suppose
$W=\mathbf{\Phi}g(t\Lambda)\mathbf{\Phi^T}$, where $g(\cdot)$ is the spectral domain generator
function. Instead of preserving the differential or Laplacian coordinates, we may choose
to preserve wavelet coefficients of different scales after deformation. The spectral expansion
of the optimization problem then becomes
\begin{equation}
\label{eq:sgw-editing}
\min_\mathbf{f} \sum_{j\in C} (f_j - d_j)^2 + \gamma \sum_k g(t\lambda_k) a_k^2.
\end{equation}

Clearly, changing the generator function $g(\cdot)$ or the scale parameter $t$ will
influence the contribution of components of different frequencies in the final reconstruction.

\subsection*{Sparse Representation Learning}
Our current research focuses on reconstructive sparse models with predefined dictionaries
composed of analytical functions such as wavelets. Nevertheless, sparse representations have
also great potential for pattern recognition, as most meaningful high-dimensional
signals probably reside in some low-dimensional subspace which can be better revealed by proper
sparse models.

To construct a highly discriminative sparse models, predefined dictionaries are usually not
sufficiently expressive. To improve the sparsity and discriminative power, a dictionary learned from
the data set is much preferred. Given training signals $\{y_i\}_{i=1}^N$, a dictionary that
minimize the overall coefficient sparsity of the training signals can be formulated as follows:
\begin{equation}
\label{eq:dictionary-learning}
\hat{D}=\arg\min_D\sum_{i=1}^N \min_{x_i}\{\|Dx_i - y_i\|_2^2 + \lambda\|x_i\|_1\}.
\end{equation}
The dictionary learning can be handled by algorithms such as K-SVD~\cite{Aharon2006}.

After the dictionary $D$ is learned, for an input signal $y$, a sparse coefficient
representation $\hat{x}$ can be easily computed with respect $D$ using
common $l_1$ formulation:
\begin{equation}
\hat{x} = \arg\min_{x} \{\|Dx - y\|_2^2 + \lambda\|x\|_1\}.
\end{equation}

For shape signals, we cannot directly apply the dictionary learning formulation in
Eq.~\ref{eq:laplace-editing}, since different models generally have vastly different
graph structures and cannot be easily corresponded. Nevertheless, we can design feature
descriptors as a proxy instead of processing shape signals directly. A highly discriminative
sparse model in the descriptor space has great potential in classification and retrieval of
shape features or whole shapes.

\subsection*{General Graph Signal Analysis}

